# ğŸ™ï¸ Speaker Diarization & Transcription Pipeline
**Whisper + VAD + Speaker Clustering**

An **end-to-end Python pipeline** for **speaker diarization** and **speech transcription**, designed to answer the core question:

> **Who spoke, when, and what was said?**

This project processes raw audio files and produces **speaker-labeled transcripts** using modern speech processing techniques such as **Voice Activity Detection (VAD)**, **speaker embedding clustering**, and **OpenAI Whisper** for transcription.

---

## ğŸš€ Features

### ğŸ”Š Audio Preprocessing
- Audio format conversion & resampling
- Noise reduction & normalization
- Filtering and denoising

### ğŸ¯ Voice Activity Detection (VAD)
- Accurate speech/silence segmentation
- Powered by **SpeechBrain**

### ğŸ‘¥ Speaker Diarization
- Speaker embedding extraction
- Clustering-based speaker segmentation
- Detects **who spoke when**
- Configurable support for overlapping speech

### ğŸ“ Speech Transcription
- High-accuracy transcription using **OpenAI Whisper**
- Supports multiple Whisper model sizes (`tiny` â†’ `large`)

### ğŸ–¥ï¸ Interactive UI
- Streamlit-based web interface
- Adjustable preprocessing & diarization parameters
- Real-time transcription preview

---

## ğŸ§  Use Cases
- ğŸ§ Meeting & interview transcription
- ğŸ“ Call-center audio analysis
- ğŸ™ï¸ Podcast & panel discussion processing
- ğŸ§ª Speech research & experimentation
- ğŸ“Š Dataset preparation for speech ML models

---

## ğŸ› ï¸ Tech Stack
- **Python**
- **OpenAI Whisper** (ASR)
- **SpeechBrain** (VAD & embeddings)
- **Clustering-based diarization**
- **Streamlit**
- **Audio signal processing**

---

## ğŸ“ Project Structure

```text
.
â”œâ”€â”€ app.py                     # Streamlit application
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ speaker_id_pipeline.ipynb   # Development & experiments
â”œâ”€â”€ audio_samples/             # Example input audio files
â”œâ”€â”€ output/                    # Transcripts & diarization output
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md
```

## âš™ï¸ Installation

1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-username/speaker-diarization-pipeline.git
cd speaker-diarization-pipeline
```

2ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```
âš ï¸ Note: GPU acceleration will be used automatically if CUDA is available.

â–¶ï¸ Running the Application
Launch the Streamlit app:
```bash
streamlit run app.py
```
Then open the provided local URL in your browser.

---

## ğŸ›ï¸ Configuration Options
- ğŸ¤ Whisper model selection (tiny, base, small, medium, large)
- ğŸ”Š VAD sensitivity control
- ğŸ‘¥ Speaker count range
- âš¡ GPU acceleration (if available)
- ğŸ§© Modular pipeline for easy extension

## ğŸ“¤ Example Output
```bash
[00:00:02 - 00:00:06] Speaker 1: Hello everyone, welcome to the meeting.
[00:00:07 - 00:00:12] Speaker 2: Thanks, letâ€™s get started.
```

# Output Includes
- Speaker-labeled transcripts
- Timestamps
- Structured text ready for downstream processing
